---
version:
- Server v2.x
- Server Admin
---
= Monitoring Your Installation
:page-layout: classic-docs
:page-liquid:
:icons: font
:toc: macro
:toc-title:

This section includes information on metrics for monitoring your CircleCI Server installation.

toc::[]

== メトリクスの概要

メトリクスとは、監視と分析を目的として収集される技術的な統計データのことです。 このデータには、CPU やメモリの使用率などの基本情報のほか、実行済みビルド数や内部エラー数といった高度なデータも含まれます。 メトリクスを活用すると、以下のことが可能になります。

* インシデントや異常な動作をすばやく検出する
* コンピューティング リソースを動的にスケールする
* インフラ全体の問題をさかのぼって把握する

=== How Metrics Work in CircleCI Server

Telegraf is the main component used for metrics collection in CircleCI Server. Telegraf is server software that brokers metrics data emitted by CircleCI services to data monitoring platforms such as Datadog or AWS CloudWatch.

.メトリクス
image::metrics.png[メトリクス名s]

CircleCI Server では、以下のようにしてメトリクスが収集されます。

* Each component of a Server installation sends metrics data to the telegraf container running on the Services machine.
* Telegraf がポート 8125/UDP をリッスンし、全コンポーネントからのデータを受信 (入力) した後、事前に構成されたフィルターを適用してデータを保持するか破棄するかを判断する。
* For some metric-types, Telegraf keeps metrics data inside and calculates statistical data (such as max, min, mean, stdev, sum) periodically.
* Finally, Telegraf sends out data to configured sinks (outputs), such as stdout (on the Services machine), Datadog and/or AWS CloudWatch.

It is worth noting that Telegraf can accept multiple input and output types at the same time allowing administrators to configure a single Telegraf instance to collect and forward multiple metrics data sets to both Datadog and CloudWatch.

== 標準のメトリクス構成

Review your metrics configuration file using the following command:

ifndef::pdf[{% raw %}]
```sh
sudo docker inspect --format='{{range .Mounts}}{{println .Source "->" .Destination}}{{end}}' telegraf | grep telegraf.conf | awk '{ print $1 }' | xargs cat
```
ifndef::pdf[{% endraw %}]

There are four notable blocks in the file (some blocks might not be there depending on your configuration in the Management Console):

* `\[[inputs.statsd]]` – Input configuration to receive metrics data through 8125/UDP (as discussed above)
* `\[[outputs.file]]` – Output configuration to emit metrics to stdout. All accepted metrics are configured to be shown in Telegraf docker logs. This is helpful for debugging your metrics configuration.
* `\[[outputs.cloudwatch]]` – Output configuration to emit metrics to CloudWatch
* `\[[outputs.datadog]]` – Output configuration to emit metrics to Datadog

This configuration file is automatically generated by Replicated (the service used to manage and deploy CircleCI Server) and is fully managed by Replicated. If you wish to customize the standard configuration you will need to configure Replicated to **not** insert the blocks you want to change. 

CAUTION: Do not attempt to directly modify the file. Any changes made in this way will be destroyed by Replicated upon certain events, such as service restarts. For example, if a customized `\[[inputs.statsd]]` block is added _without_ stopping automatic interpolation, you will encounter errors as Telegraf attempts to listen to `8125/UDP` twice, and the second attempt will fail with `EADDRINUSE`.

In a standard configuration with no metrics customization the main config discussed above is all that is required. If you have configured metrics customization by placing files under `/etc/circleconfig/telegraf`, those configurations are appended to the main config – imagine `cat`ing the main config and all of those customization files. For more on customizing metrics see the <<custom-metrics>> section.

== システム監視のメトリクス

To enable metrics forwarding to either AWS Cloudwatch or Datadog, follow the steps for the service you wish to use in the <<supported-platforms>> section. 以下のセクションでは、CircleCI Server 環境で利用可能なメトリクスの概要を説明します。

=== VM サービスと Docker のメトリクス

VM サービスと Docker サービスのメトリクスは、メトリクスの収集とレポート用プラグイン駆動型サーバー エージェントである https://github.com/influxdata/telegraf[Telegraf] によって転送されます。

以下のメトリクスが有効化されています。

* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/cpu/README.md#cpu-time-measurements[CPU]
* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/disk/README.md#metrics[ディスク]
* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/mem/README.md#metrics[メモリ]
* https://github.com/influxdata/telegraf/blob/master/plugins/inputs/net/NET_README.md[ネットワーク通信]
* https://github.com/influxdata/telegraf/tree/master/plugins/inputs/docker#metrics[Docker]

=== Nomad ジョブのメトリクス

https://www.nomadproject.io/docs/telemetry/metrics.html#job-metrics[Nomad ジョブのメトリクス]は、Nomad サーバー エージェントによって有効化され、送信されます。 以下の 5種類のメトリクスが報告されます。

[.table.table-striped]
[cols=2*, options="header", stripes=even]
[cols="6,5"]
|===
|Metric
|お問い合わせ内容

|`circle.nomad.server_agent.poll_failure`
|Nomad エージェントの最後のポーリングが失敗した場合は 1、成功した場合は 0 を返します。

|`circle.nomad.server_agent.jobs.pending`
|クラスター全体の保留中ジョブの総数を返します。

|`circle.nomad.server_agent.jobs.running`
|クラスター全体の実行中ジョブの総数を返します。

|`circle.nomad.server_agent.jobs.complete`
|クラスター全体の完了済みジョブの総数を返します。

|`circle.nomad.server_agent.jobs.dead`
|クラスター全体の停止中ジョブの総数を返します。
|===

Nomad メトリクスのコンテナが正常に動作している場合、標準出力や標準エラーには何も出力されません。 障害が発生すると、標準エラーにメッセージが出力されます。

=== CircleCI のメトリクス
_Introduced in CircleCI Server v2.18_

[.table.table-striped]
[cols=2*, stripes=even]
[cols="5,6"]
|===
| `circle.backend.action.upload-artifact-error`
| アーティファクトのアップロードに失敗した回数

| `circle.build-queue.runnable.builds`
| システムを経由するビルドのうち実行可能と見なされているビルドの数

| `circle.dispatcher.find-containers-failed`
| 1.0 ビルドの数

| `circle.github.api_call`
| CircleCI が GitHub に対して実行している API 呼び出しの回数

| `circle.http.request`
| CircleCi のリクエストへの応答コード

| `circle.nomad.client_agent.*``
| Nomad クライアントのメトリクス

| `circle.nomad.server_agent.*`
| 存在する Nomad サーバーの数

| `circle.run-queue.latency`
| 実行可能なビルドが待機している時間

| `circle.state.container-builder-ratio`
| Builder ごとのコンテナの数 (1.0 のみ)

| `circle.state.lxc-available`
| 利用可能なコンテナの数 (1.0 のみ)

| `circle.state.lxc-reserved`
| 予約/使用中のコンテナの数 (1.0 のみ)

| `circleci.cron-service.messaging.handle-message`
| `cron-service` によって処理される RabbitMQ メッセージのタイミングと数

| `circleci.grpc-response`
| grpc システムが呼び出すシステムの待機時間
|===

// There are a couple of nomad metrics in this table... they should maybe be moved to the section above? ^^

// Taken out of table until told otherwise
//| `Circle.vm-service.vm.assigned-vm`
// | Tracks how many vm’s are in use.

// | `Circle.vm-service.vms.delete.status`
// | Tracks how many vm’s we’re deleting at a given moment.

// | `Circle.vm-service.vms.get.status`
// | TBD (Tracks how many vm’s we have?)

// | `Circle.vm-service.vms.post.status`
// | TBD
<<<

== サポート対象プラットフォーム

We have two built-in platforms for metrics and monitoring: AWS CloudWatch and DataDog. The sections below detail enabling and configuring each in turn.

=== AWS CloudWatch

To enable AWS CloudWatch complete the following:

1. Navigate to the settings page within your Management Console. You can use the following URL, substituting your CircleCI URL: `your-circleci-hostname.com:8800/settings#cloudwatch_metrics`.

2. Check Enabled under AWS CloudWatch Metrics to begin configuration.
+
.Enable Cloudwatch
image::metrics_aws_cloudwatch1.png[AWS CloudWatch]

==== AWS CloudWatch Configuration

設定には、2つのオプションがあります。

* Services box の [IAM Instance Profile (IAM インスタンスプロファイル)] を使用し、カスタムの領域と名前空間を設定する方法
+
.CloudWatch Region and Namespace
image::metrics_aws_cloudwatch2a.png[Configuration IAM]

* カスタムの領域と名前空間と共に、AWS のアクセスキーとシークレットキーを使用する方法
+
.Access Key and Secret Key
image::metrics_aws_cloudwatch2b.png[Configuration Alt]

After saving you can *verify* that metrics are forwarding by going to your AWS CloudWatch console.

=== Datadog

To enable Datadog complete the following:

// 1. Disable Telegraf - at this time both Datadog and Telegraf require port 8125
. Navigate your Management Console Settings. You can use the following URL, substituting your CircleCI hostname: `your-circleci-hostname.com:8800/settings#datadog_metrics`

. Check Enabled under Datadog Metrics to begin configuration.
+
.Enable Datadog Metrics
image::metrics_datadog1.png[Enable DataDog]

. Datadog API キーを入力します。 You can verify that metrics are forwarding by going to your DataDog metrics summary.
+
.Enter Datadog API key
image::metrics_datadog2.png[DataDog API Key]

== カスタムメトリクス

Custom Metrics using a Telegraf configuration file allows for more fine grained control than allowing Replicated to forward standard metrics to Datadog or AWS Cloudwatch.

The basic Server metrics configuration assumes fundamental use cases only. It might be beneficial to customize the way metrics are handled for your installation in the following ways:

* Forward metrics data to your preferred platform (e.g. your own InfluxDB instance)
* Monitor additional metrics in order to detect specific events
* Reduce the number of metrics sent to data analysis platforms (to reduce gross operation costs)

=== 1. Disable Standard Metrics Setup

Disable Replicated's interpolation of the Telegraf configuration to fully customize [[inputs.statsd]] and outputs:

. Open the Management Console.
. On the **Settings** page, go to **Custom Metrics** section and enable the "Use custom telegraf metrics" option.
+
.カスタムメトリクス
image::custom_metrics.png[Custom Metrics]
. Scroll down to save the change and restart services.

NOTE: There will be a downtime along with a service restart. After disabling it you will have to manually configure outputs to Datadog and/or CloudWatch, regardless of configurations on Replicated.

=== 2. Create your Customized Config

Now you are ready to do anything Telegraf supports! All you need to provide is a valid Telegraf config file.

. Services マシンに SSH で接続します。
. Add the following to `/etc/circleconfig/telegraf/statsd.conf`
+
```
[[inputs.statsd]]
        service_address = ":8125"
        parse_data_dog_tags = true
        metric_separator = "."
        namepass = []
```
. Under `namepass` add any metrics you wish to receive, the example below shows choosing to configure just the first 4 from the list above. (See below for some additional example configs):
+
```
[[inputs.statsd]]
        service_address = ":8125"
        parse_data_dog_tags = true
        metric_separator = "."
        namepass = [
            "circle.backend.action.upload-artifact-error",
            "circle.build-queue.runnable.builds",
            "circle.dispatcher.find-containers-failed",
            "circle.github.api_call"
          ]
```
. Restart the telegraf container by running: `sudo docker restart telegraf`

NOTE: See the https://github.com/influxdata/telegraf/blob/master/README.md[Telegraf README] for further config syntax details.

[discrete]
==== Sample Telegraph Configuration

[discrete]
===== Scenario 1: Record standard metrics to two InfluxDB instances

The example below records default metrics to two InfluxDB instances: One is your on-premises InfluxDB server (`your-influx-db-instance.example.com`), and the other is https://cloud2.influxdata.com/[InfluxDB Cloud 2].

```
[[inputs.statsd]]
  service_address = ":8125"
  parse_data_dog_tags = true
  metric_separator = "."
  namepass = [
    "circle.backend.action.upload-artifact-error",
    "circle.build-queue.runnable.builds",
    "circle.dispatcher.find-containers-failed",
    "circle.github.api_call",
    "circle.http.request",
    "circle.nomad.client_agent.*",
    "circle.nomad.server_agent.*",
    "circle.run-queue.latency",
    "circle.state.container-builder-ratio",
    "circle.state.lxc-available",
    "circle.state.lxc-reserved",
    "circle.vm-service.vm.assigned-vm",
    "circle.vm-service.vms.delete.status",
    "circle.vm-service.vms.get.status",
    "circle.vm-service.vms.post.status",
    "circleci.cron-service.messaging.handle-message",
    "circleci.grpc-response"
  ]

[[outputs.influxdb]]
  url = "http://your-influx-db-instance.example.com:8086"
  database = "circleci"

[[outputs.influxdb_v2]]
  urls = ["https://us-central1-1.gcp.cloud2.influxdata.com"]
  token = "YOUR_TOKEN_HERE"
  organization = "circle@example.com"
  bucket = "circleci"
```

[discrete]
===== Scenario 2: Record all metrics to Datadog

The standard configuration handles only selected metrics, and there are many metrics discarded by Telegraf. If you want to receive this discarded, sophisticated data, such as JVM stats and per-container CPU usage, you can keep all received metrics by removing namepass filter. This example also illustrates how to configure metrics emission to Datadog. As discussed above, you need manual configuration for outputs to Datadog regardless of configurations on Replicated.

CAUTION: This scenario leads to very large amounts of data.

```
[[inputs.statsd]]
  service_address = ":8125"
  parse_data_dog_tags = true
  metric_separator = "."

[[outputs.datadog]]
  apikey = 'YOUR_API_KEY_HERE'
```

[discrete]
===== Scenario 3: Send limited metrics to CloudWatch

AWS charges fees for CloudWatch per series of scalar (i.e. at the level of "mean" or "sum"). Since multiple fields (e.g. mean, max, min and sum) are calculated for each metrics key (e.g. `circle.run-queue.latency`) and some fields can be redundant, you might want to select which fields are sent to CloudWatch. This can be achieved by configuring `\[[outputs.cloudwatch]]` with `fieldpass`. You also may declare `\[[outputs.cloudwatch]]` multiple times to pick up multiple metrics, as illustrated below.

```
[[inputs.statsd]]
  # Accept all metrics at input level to 1) enable output configurations without thinking of inputs, and to 2) dump discarded metrics to stdout just in case.
  service_address = ":8125"
  parse_data_dog_tags = true
  metric_separator = "."

[[outputs.cloudwatch]]
    # Fill in these two variables if you need to access CloudWatch with an IAM User, not an IAM Role attached to your Services box
    # access_key = 'ACCESS'
    # secret_key = 'SECRET'

    # Specify region for CloudWatch
    region = 'ap-northeast-1'
    # Specify namespace for easier monitoring
    namespace = 'my-circleci-server'

    # Name of metrics key to record
    namepass = ['circle.run-queue.latency']
    # Name of metrics field to record; key and field are delimited by an underscore (_)
    fieldpass = ['mean']

[[outputs.cloudwatch]]
    # Outputs can be specified multiple times.

    # Fill in these two variables if you need to access CloudWatch with an IAM User, not an IAM Role attached to your Services box
    # access_key = 'ACCESS'
    # secret_key = 'SECRET'

    # Specify region for CloudWatch
    region = 'ap-northeast-1'
    # Specify namespace for easier monitoring
    namespace = 'my-circleci-server'

    # Name of metrics key to record
    namepass = ['mem']
    # Name of metrics field to record; key and field are delimited by an underscore (_)
    fieldpass = ['available_percent']
```

== Additional Tips

`docker logs -f telegraf` を実行してログをチェックすることで、設定した出力に出力プロバイダー (influx など) がリストされているかどうかを確認できます。 また、インストール内のすべてのメトリクスが環境に対してタグ付けされるようにするには、コンフィグ内に以下のコードを記述します。

```yaml
[global_tags]
Env="<staging-circleci>"
```

Please see the InfluxDB https://github.com/influxdata/influxdb#installation[documentation] for default and advanced installation steps.

CAUTION: Any changes to the config will require a restart of the CircleCI application which will require downtime.

// Extra Metics info not currently included
////
### Datadog Dashboard Configuration

This section shows you how to set up a Datadog dashboard for CircleCI metrics. We also provide descriptions of the metrics we currently support.

NOTE: CircleCI metrics are subject to change. The names of individual metrics may change, as well as their scope and monitoring options. Any changes will take place along with our usual release cycle and will be flagged up in our Changelog**

\newpage

#### The dashboard

Below is an image of our Datadog dashboard showing graphs for Make Workflow, Run queue, Time to complete Workflow, Count of Workflows completed by Status, and Build Service Latency.

![DataDog Dashboard](images/datadog-0.png)

#### JSON dashboard creation

The following JSON is for the dashboard shown above. You can use this to build the dashboard for your CircleCI Server installation:

\pagebreak

\tiny

```
{
   "notify_list":null,
   "description":"created by support@circleci.com",
   "template_variables":[

   ],
   "is_read_only":false,
   "id":"b44-4vy-w6r",
   "title":"Critical Path: Jobs",
   "url":"/dashboard/b44-4vy-w6r/critical-path-customer-builds",
   "created_at":"2018-10-25T07:28:08.108516+00:00",
   "modified_at":"2019-03-19T08:54:28.109067+00:00",
   "author_handle":"paulrobinson@circleci.com",
   "widgets":[
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.messaging.make_workflow.time_since_push.avg{*}",
                  "style":{
                     "line_width":"normal",
                     "palette":"warm",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               },
               {
                  "q":"max:workflows_conductor.messaging.make_workflow.time_since_push.median{*}",
                  "style":{
                     "line_width":"normal",
                     "palette":"cool",
                     "line_type":"solid"
                  },
                  "display_type":"area"
               }
            ],
            "type":"timeseries",
            "title":"Make Workflow: Time since push (mean/median) (ms)"
         },
         "id":380774989
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.messaging.make_workflow.time_since_push.95percentile{*}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Make Workflow: Time since push (95th percentile - ms)"
         },
         "id":395803486
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"avg:circle.run_queue.latency.avg{platform:picard}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Run queue: Time to job started (avg) ms"
         },
         "id":381397080
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.avg{*} by {status}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"area"
               },
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.median{*} by {status}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "yaxis":{
               "include_zero":false
            },
            "type":"timeseries",
            "title":"Time to complete workflow Mean/Median in ms (Success/Failure/Error)"
         },
         "id":395476806
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.95percentile{*} by {status}",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "yaxis":{
               "include_zero":false
            },
            "type":"timeseries",
            "title":"Time to complete workflow 95th percentile ms (Success/Failure/Error)"
         },
         "id":395804031
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:workflows_conductor.execute_workflow.time_to_complete.count{*} by {status}.as_count()",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Count of workflows completed by Status"
         },
         "id":393871870
      },
      {
         "definition":{
            "requests":[
               {
                  "q":"max:builds_service.service.process_build.max{*}.rollup(max)",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               },
               {
                  "q":"avg:builds_service.service.process_build.median{*}.rollup(avg)",
                  "style":{
                     "line_width":"normal",
                     "palette":"dog_classic",
                     "line_type":"solid"
                  },
                  "display_type":"line"
               }
            ],
            "type":"timeseries",
            "title":"Build Service Latency (time to process a build)"
         },
         "id":3833057922780384
      }
   ],
   "layout_type":"ordered"
}
```

\normalsize

#### The Metrics

Following are descriptions of the specific metrics related to workflows, followed by dashboard screengrabs with those metrics highlighted:

`workflows_conductor.messaging.make_workflow.time_since_push.avg` (gauge)

* Average time from a trigger (GitHub hook) entering CircleCI and the workflow being created, shown in milliseconds.

<!--- `workflows_conductor.execute_workflow.time_to_complete.median` (gauge): Median time to execute a workflow, shown in milliseconds.--->

<!--`workflows_conductor.execute_workflow.time_to_complete.avg` (gauge)

* Average time to execute a workflow, shown in milliseconds.

![workflows_conductor.messaging.make_workflow.time_since_push.avg (gauge) Average time to make a workflow](images/datadog-1.png)

<!---![workflows_conductor.execute_workflow.time_to_complete.median (gauge): Median time to execute a workflow, shown in milliseconds](images/datadog-2.png)--->

<!---[workflows_conductor.messaging.make_workflow.time_since_push.median (gauge): Median time to make a workflow, shown as millisecond](images/datadog-3.png)--->

<!--![workflows_conductor.execute_workflow.time_to_complete.avg (gauge): Average time to execute a workflow](images/datadog-4.png)

\pagebreak

## Monitoring Tasks

The following section describes actions to take when a threshold is exceeded for a monitored metric, for the Workflows, API-service, Nomad, or VM service.

### Workflows

#### Workflow message timing outliers

`workflows_conductor.engine_handler.messages.timing.95percentile`

**Notes/Actions**: This metric is a good indicator that work is proceeding in a timely manner. If timing threshold is exceeded, complete the following steps:

1. Check `workflows-conductor` logs. If logging isn't happening, restart.
2. Check for exceptions from the workflows-conductor containers.

#### Number of messages received

`workflows_conductor.engine_handler.messages.timing.count`

**Notes/Actions**: This metric is a good indicator that work is flowing through the system. If message count drops to zero, complete the following steps:

1. Restart the `workflows-conductor` container
2. Check `workflows-conductor` logs. If logging isn't happening, restart
3. Check Github webhooks are being recieved to trigger jobs
4. Check for exceptions from `workflows-conductor` or `frontend` containers

#### Average time taken for Workflows to complete

`workflows_conductor.execute_workflow.time_to_complete.avg`

**Notes/Actions**: Some variation here is expected due to fluctuations in job and usage queue times. If threshold is exceeded, complete the following steps:

1. Check `workflows-conductor` logs. If logging isn't happening, restart.
2. Check `domain-service` logs. If logging isn't happening, restart.
3. Check `contexts-service` logs. If logging isn't happening, restart.
4. Check `permissions-service` logs. If logging isn't happening, restart.
5. Check for exceptions from `workflows-conductor`, `domain-service`, `contexts-service` and `permissions-service` containers.

<!--- `workflows_conductor.execute_workflow.time_to_complete.median`
Indicates TBD, if threshold is exceeded, complete the following steps:
1. TBD
2. TBD
3. TBD--->

<!--#### Workflows conductor memory used

`jvm.memory.total.used`

**Tag filter**: `service:workflows-conductor`

**Notes/Actions**: Indicates the amount of memory used by the Workflows Conductor service. If threshold is exceeded restart the `workflows-conductor`

\pagebreak

### API-service

The following metrics can be inspected to get diagnostic information on how the API service is running.

#### Average API response time

`backplane.ring.http_request.avg`

**Tag filter**: `service:api-service`

**Notes/ Actions**: Indicates the average response time from the API is increasing.

#### Number of API requests

`backplane.ring.http_request.count`

**Tag filter**: `service:api-service`

**Notes/Actions**: Indicates a high number of API requests.

#### Maximum time to return an API response

`backplane.ring.http_request.max`

**Tag filter**: `service:api-service`

#### Slow API response speed

`backplane.ring.http_request.95percentile`

**Tag filter**: `service:api-service`

#### Number of active threads in the JVM

`jvm.thread.count`

**Tag filter**: `service:api-service`

**Notes/Actions**: If this count goes above 1000, set `DOMAIN_SERVICE_REFRESH_USERS` environment variable to `false`.

#### GraphQL Resolver

`circleci.api_service.graphql.resolver.avg`

**Tag filter**: `service:api-service`

**Notes/Actions**: This metric can be split up using `type` tags to determine downstream service issues. If the threshold is exceeded across types, complete the following steps:

1. Take a thread dump of the api-service
2. Restart
3. Supply the thread dump with any tickets

If the slowdown is only for a subset of types, then inspect metrics for the corresponding service.

### Nomad

#### Average latency of builds in queue

`circle.run_queue.latency.avg`

**Notes/Actions**: Captures backup between CircleCI and Nomad. If threshold is exceeded, add additional capacity to Nomad or your VM pool.

## Monitor Settings

This section describes threshold settings for the Nomad, Domain, Workflows and VM Service to monitor common failure conditions and checks or corrective actions for each condition.

### Nomad

#### More than 10 recent jobs failed on {host}

`sum(last_10m):sum:build_agent.infra_failed{env:prod} by {host}.as_count() > 10`

**Notes/Actions**: This may indicate a bad host.

#### A number of builds are queued due to Nomad capacity

```
min(last_10m):avg:circle.run_queue.latency.avg /
{env:production,platform:picard} > 65000
```

**Notes/Actions**: Scale up the number of Nomad clients.

### Domain Service

#### Error rate increased

\footnotesize

```
avg(last_5m):default(sum:circle.domain_service.users.id.get.status{!status:200,!status:202}.as_count(), 0) /
default(sum:circle.domain_service.users.id.get.status{*}.as_count(), 0) >= 0.5
```
\normalsize

**Notes/Actions**: This might indicate problems with GitHub, check for exceptions in `domain-service` logs.

### Permissions Service

#### Error rate increased

\footnotesize

```
avg(last_5m):( default(sum:circle.permissions_service.permissions.get.status{status:500}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:502}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:503}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:504}.as_count(), 0) ) /
( default(sum:circle.permissions_service.permissions.get.status{status:200}.as_count(), 0)
+ default(sum:circle.permissions_service.permissions.get.status{status:202}.as_count(), 0) ) >= 0.2
```

\normalsize

**Notes/Actions**: This might indicate problems with `domain-service`, check for exceptions in `permissions-service` and `domain-service` logs.

### Workflows

#### gRPC error rate is elevated

```
avg(last_10m):sum:grpc_response.count /
{service:workflows-conductor,!status:ok}.as_count() /
sum:grpc_response.count{service:workflows-conductor}.as_count() > 0.2
```

**Notes/Actions**: Check for exceptions from `workflows-conductor`, `domain-service`, `contexts-service` and `permissions-service`.

#### No scheduled workflows have run in the last 5 minutes

```
sum(last_5m):sum:workflows_conductor.trigger.decision /
{decision:success}.as_count() < 1
```

**Notes/Actions**: Perform the following corrective actions:

1. Check `cron-service` logs. If logging isn't happening, restart.
2. Check for exceptions from `cron-service` and `workflows-conductor`.

### VM Service

#### VM service is responding with 5x errors
\footnotesize

```
sum(last_1m):sum:circle.vm_service.vms.get.status /
{status:500}.as_count() + /
sum:circle.vm_service.vms.get.status{status:503}.as_count() + /
sum:circle.vm_service.vms.get.status{status:504}.as_count() + /
sum:circle.vm_service.vms.post.status{status:500}.as_count() + /
sum:circle.vm_service.vms.post.status{status:504}.as_count() + /
sum:circle.vm_service.vms.delete.status{status:500}.as_count() + /
sum:circle.vm_service.vms.delete.status{status:503}.as_count() + /
sum:circle.vm_service.vms.delete.status{status:504}.as_count() > 3
```
\normalsize

**Notes/Actions**: Check VM service metrics to identify root cause.

#### Multiple VM service provisioning errors

```
sum(last_10m):sum:build_agent.machine.created.count /
{result:error} by {resource_class_id}.as_count() > 50
```

**Notes/Actions**: This may be indicative of an issue like rate-limiting.

#### VM machine provisioning taking too long
\footnotesize

```
avg(last_5m):avg:build_agent.machine.created.avg /
{result:succeeded,resource_class_id:l1.medium, /
!docker_layer_caching:true} > 180000
```

\normalsize

**Notes/Actions**: Check VM service metrics to look for potential problems (this monitor could also be related to disk IOPS contention).-->
////
